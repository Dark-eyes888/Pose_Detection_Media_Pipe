{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import mediapipe as mp\n",
    "import numpy as np \n",
    "import cv2 as cv\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing= mp.solutions.drawing_utils # drawing helper\n",
    "mp_pose= mp.solutions.pose # mediapipe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resize(frame,scale):\n",
    "            height=int(frame.shape[0]* scale)\n",
    "            width=int(frame.shape[1]*scale)\n",
    "            dimensions=(width,height)\n",
    "\n",
    "            return cv.resize(frame,dimensions,interpolation=cv.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting pose landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Camera feed\n",
    "\n",
    "video=cv.VideoCapture(0)\n",
    "\n",
    "with mp_pose.Pose(model_complexity=2,min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "    # set time \n",
    "    time1= 0\n",
    "\n",
    "\n",
    "    while video.isOpened():\n",
    "        ret, frame= video.read()\n",
    "        frame.flags.writeable=False\n",
    "        re_img=Resize(frame,1.5)\n",
    "\n",
    "        # change color feed\n",
    "        \n",
    "        img=cv.cvtColor(re_img,cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        img=cv.flip(img,1)\n",
    "        # make detections\n",
    "        results=pose.process(img)\n",
    "\n",
    "        time2= time.time()\n",
    "\n",
    "        if (time2 - time1)>0:\n",
    "            \n",
    "            fps=round(1.0/(time2-time1),1)\n",
    "\n",
    "            cv.putText(img,f'FPS : {fps}',(10,30),cv.FONT_HERSHEY_PLAIN,2,(0,255,0),2)\n",
    "        \n",
    "\n",
    "        time1=time2\n",
    "        img.flags.writeable=True\n",
    "        img =cv.cvtColor(img,cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # Pose Detections\n",
    "        mp_drawing.draw_landmarks(img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,mp_drawing.DrawingSpec(color=(178, 247, 31),thickness=2,circle_radius=1),mp_drawing.DrawingSpec(color=(129, 102, 108),thickness=1))\n",
    "\n",
    "        \n",
    "        cv.imshow('Raw Webcam Feed', img)\n",
    "\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genearating Dataset for yoga poses consisting 4 classes - Downdog,goddess,plank,Tree,Warrior2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects=len(results.pose_landmarks.landmark)\n",
    "yoga=['class']\n",
    "yoga += list((np.array([[f'x_{i}',f'y_{i}',f'z_{i}',f'v_{i}'] for i in range(1,num_objects+1)]).flatten()))\n",
    "with open('landmarks.csv','w',newline='') as f:\n",
    "    file=csv.writer(f,delimiter=',')\n",
    "    file.writerow(yoga)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pose_name=os.listdir(r'C:\\Users\\adm88\\ML_deployment\\Object_detection\\yoga_pose_media_pipe\\TEST')\n",
    "\n",
    "with mp_pose.Pose(model_complexity=2,static_image_mode=True,min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    for i in Pose_name:\n",
    "        class_name=i\n",
    "        img_dir_path=r'C:\\Users\\adm88\\ML_deployment\\Object_detection\\yoga_pose_media_pipe\\TEST\\{0}'.format(i)\n",
    "        for j in os.listdir(img_dir_path):\n",
    "            pic=cv.imread(img_dir_path+\"\\\\\"+j)\n",
    "\n",
    "            # detecting poses\n",
    "            \n",
    "            pic=cv.cvtColor(pic,cv.COLOR_BGR2RGB)\n",
    "            pic=cv.resize(pic,(540,540),cv.INTER_LINEAR)\n",
    "        \n",
    "            \n",
    "            # make detections\n",
    "            result=pose.process(pic)\n",
    "             # change color feed\n",
    "        \n",
    "            pic=cv.cvtColor(pic,cv.COLOR_RGB2BGR)\n",
    "            \n",
    "\n",
    "            # Pose Detections\n",
    "            \n",
    "            mp_drawing.draw_landmarks(pic, result.pose_landmarks, mp_pose.POSE_CONNECTIONS,mp_drawing.DrawingSpec(color=(178, 247, 31),thickness=2,circle_radius=1),mp_drawing.DrawingSpec(color=(129, 102, 108),thickness=1))\n",
    "            \n",
    "            Class_landmarks= result.pose_landmarks.landmark\n",
    "            landmarks= list(np.array([[i.x,i.y,i.x,i.visibility] for i in Class_landmarks]).flatten())\n",
    "            landmarks.insert(0,class_name)\n",
    "\n",
    "            with open('landmarks.csv','a',newline='') as f:\n",
    "                File= csv.writer(f, delimiter=',')\n",
    "                File.writerow(landmarks)\n",
    "                \n",
    "            \n",
    "            #cv.imshow('Raw Feed', pic)\n",
    "\n",
    "            cv.waitKey(1) \n",
    "            \n",
    "            cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding custom data with webcam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adm88\\ML_deployment\\Object_detection\\yoga_pose_media_pipe\\pose.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000020?line=11'>12</a>\u001b[0m pic\u001b[39m=\u001b[39mcv\u001b[39m.\u001b[39mresize(pic,(\u001b[39m540\u001b[39m,\u001b[39m540\u001b[39m),cv\u001b[39m.\u001b[39mINTER_LINEAR)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000020?line=14'>15</a>\u001b[0m     \u001b[39m# make detections\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000020?line=15'>16</a>\u001b[0m result\u001b[39m=\u001b[39mpose\u001b[39m.\u001b[39;49mprocess(pic)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000020?line=16'>17</a>\u001b[0m      \u001b[39m# change color feed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000020?line=17'>18</a>\u001b[0m pic\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adm88\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adm88\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solution_base.py:364\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    358\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    360\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    361\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    362\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 364\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    365\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    367\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    368\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class_name= 'Normal'\n",
    "cap=cv.VideoCapture(0)\n",
    "with mp_pose.Pose(model_complexity=2,static_image_mode=True,min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        _, frame=cap.read()\n",
    "        frame.flags.writeable=False\n",
    "\n",
    "            # detecting poses\n",
    "        \n",
    "        pic=cv.cvtColor(frame,cv.COLOR_BGR2RGB)\n",
    "        pic=cv.resize(pic,(540,540),cv.INTER_LINEAR)\n",
    "        \n",
    "            \n",
    "            # make detections\n",
    "        result=pose.process(pic)\n",
    "             # change color feed\n",
    "        pic.flags.writeable=True\n",
    "        pic=cv.cvtColor(pic,cv.COLOR_RGB2BGR)\n",
    "            \n",
    "\n",
    "            # Pose Detections\n",
    "            \n",
    "        mp_drawing.draw_landmarks(pic, result.pose_landmarks, mp_pose.POSE_CONNECTIONS,mp_drawing.DrawingSpec(color=(178, 247, 31),thickness=2,circle_radius=1),mp_drawing.DrawingSpec(color=(129, 102, 108),thickness=1))\n",
    "            \n",
    "        Class_landmarks= result.pose_landmarks.landmark\n",
    "        landmarks= list(np.array([[i.x,i.y,i.x,i.visibility] for i in Class_landmarks]).flatten())\n",
    "        landmarks.insert(0,class_name)\n",
    "\n",
    "        with open('landmarks.csv','a',newline='') as f:\n",
    "            File= csv.writer(f, delimiter=',')\n",
    "            File.writerow(landmarks)\n",
    "                \n",
    "            \n",
    "            cv.imshow('Raw Feed', pic)\n",
    "\n",
    "        cv.waitKey(1) \n",
    "cap.release()          \n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.5981493592262268\n",
       "y: 0.38991042971611023\n",
       "z: -1.320794939994812\n",
       "visibility: 0.9997200965881348"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>v_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>z_2</th>\n",
       "      <th>v_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>...</th>\n",
       "      <th>z_31</th>\n",
       "      <th>v_31</th>\n",
       "      <th>x_32</th>\n",
       "      <th>y_32</th>\n",
       "      <th>z_32</th>\n",
       "      <th>v_32</th>\n",
       "      <th>x_33</th>\n",
       "      <th>y_33</th>\n",
       "      <th>z_33</th>\n",
       "      <th>v_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>downdog</td>\n",
       "      <td>0.524543</td>\n",
       "      <td>0.739313</td>\n",
       "      <td>0.524543</td>\n",
       "      <td>0.999333</td>\n",
       "      <td>0.501234</td>\n",
       "      <td>0.754761</td>\n",
       "      <td>0.501234</td>\n",
       "      <td>0.999382</td>\n",
       "      <td>0.497772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884256</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.791916</td>\n",
       "      <td>0.918564</td>\n",
       "      <td>0.791916</td>\n",
       "      <td>0.279184</td>\n",
       "      <td>0.784176</td>\n",
       "      <td>0.909822</td>\n",
       "      <td>0.784176</td>\n",
       "      <td>0.005242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>downdog</td>\n",
       "      <td>0.550666</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>0.550666</td>\n",
       "      <td>0.999724</td>\n",
       "      <td>0.572692</td>\n",
       "      <td>0.773962</td>\n",
       "      <td>0.572692</td>\n",
       "      <td>0.999440</td>\n",
       "      <td>0.575015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143413</td>\n",
       "      <td>0.925378</td>\n",
       "      <td>0.259711</td>\n",
       "      <td>0.889642</td>\n",
       "      <td>0.259711</td>\n",
       "      <td>0.151637</td>\n",
       "      <td>0.249647</td>\n",
       "      <td>0.895167</td>\n",
       "      <td>0.249647</td>\n",
       "      <td>0.940103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>downdog</td>\n",
       "      <td>0.591179</td>\n",
       "      <td>0.718564</td>\n",
       "      <td>0.591179</td>\n",
       "      <td>0.998485</td>\n",
       "      <td>0.614638</td>\n",
       "      <td>0.725264</td>\n",
       "      <td>0.614638</td>\n",
       "      <td>0.993898</td>\n",
       "      <td>0.619974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055977</td>\n",
       "      <td>0.966626</td>\n",
       "      <td>0.215033</td>\n",
       "      <td>0.876577</td>\n",
       "      <td>0.215033</td>\n",
       "      <td>0.239551</td>\n",
       "      <td>0.201332</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.201332</td>\n",
       "      <td>0.967450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>downdog</td>\n",
       "      <td>0.527703</td>\n",
       "      <td>0.742895</td>\n",
       "      <td>0.527703</td>\n",
       "      <td>0.998805</td>\n",
       "      <td>0.545204</td>\n",
       "      <td>0.766965</td>\n",
       "      <td>0.545204</td>\n",
       "      <td>0.995739</td>\n",
       "      <td>0.549692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160866</td>\n",
       "      <td>0.504536</td>\n",
       "      <td>0.276530</td>\n",
       "      <td>0.871081</td>\n",
       "      <td>0.276530</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.272185</td>\n",
       "      <td>0.882554</td>\n",
       "      <td>0.272185</td>\n",
       "      <td>0.535279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>downdog</td>\n",
       "      <td>0.383210</td>\n",
       "      <td>0.672364</td>\n",
       "      <td>0.383210</td>\n",
       "      <td>0.998244</td>\n",
       "      <td>0.357112</td>\n",
       "      <td>0.693990</td>\n",
       "      <td>0.357112</td>\n",
       "      <td>0.998268</td>\n",
       "      <td>0.352921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852095</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.735646</td>\n",
       "      <td>0.910208</td>\n",
       "      <td>0.735646</td>\n",
       "      <td>0.087242</td>\n",
       "      <td>0.727957</td>\n",
       "      <td>0.897249</td>\n",
       "      <td>0.727957</td>\n",
       "      <td>0.002657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class       x_1       y_1       z_1       v_1       x_2       y_2  \\\n",
       "0  downdog  0.524543  0.739313  0.524543  0.999333  0.501234  0.754761   \n",
       "1  downdog  0.550666  0.762332  0.550666  0.999724  0.572692  0.773962   \n",
       "2  downdog  0.591179  0.718564  0.591179  0.998485  0.614638  0.725264   \n",
       "3  downdog  0.527703  0.742895  0.527703  0.998805  0.545204  0.766965   \n",
       "4  downdog  0.383210  0.672364  0.383210  0.998244  0.357112  0.693990   \n",
       "\n",
       "        z_2       v_2       x_3  ...      z_31      v_31      x_32      y_32  \\\n",
       "0  0.501234  0.999382  0.497772  ...  0.884256  0.006587  0.791916  0.918564   \n",
       "1  0.572692  0.999440  0.575015  ...  0.143413  0.925378  0.259711  0.889642   \n",
       "2  0.614638  0.993898  0.619974  ...  0.055977  0.966626  0.215033  0.876577   \n",
       "3  0.545204  0.995739  0.549692  ...  0.160866  0.504536  0.276530  0.871081   \n",
       "4  0.357112  0.998268  0.352921  ...  0.852095  0.003518  0.735646  0.910208   \n",
       "\n",
       "       z_32      v_32      x_33      y_33      z_33      v_33  \n",
       "0  0.791916  0.279184  0.784176  0.909822  0.784176  0.005242  \n",
       "1  0.259711  0.151637  0.249647  0.895167  0.249647  0.940103  \n",
       "2  0.215033  0.239551  0.201332  0.898214  0.201332  0.967450  \n",
       "3  0.276530  0.040829  0.272185  0.882554  0.272185  0.535279  \n",
       "4  0.735646  0.087242  0.727957  0.897249  0.727957  0.002657  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\adm88\\ML_deployment\\Object_detection\\yoga_pose_media_pipe\\landmarks.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Normal', 'downdog', 'goddess', 'plank', 'warrior2'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=LabelEncoder()\n",
    "\n",
    "le=label.fit_transform(df['class'].values)\n",
    "label.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split(data,test_size,random_state):\n",
    "    x=data.drop(columns='class')\n",
    "    y=le\n",
    "\n",
    "    x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=test_size,random_state=random_state)\n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "x_train,x_test,y_train,y_test=Split(df,0.15,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaling( x_train,x_test):\n",
    "    scale= StandardScaler()\n",
    "    x_train_scaled= scale.fit_transform(x_train)\n",
    "    x_test_scaled= scale.fit_transform(x_test)\n",
    "\n",
    "    \n",
    "    return x_train_scaled,x_test_scaled \n",
    "\n",
    "x_train_scaled, x_test_scaled = Scaling(x_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb=XGBClassifier(tree_method='gpu_hist').fit(x_train,y_train)\n",
    "\n",
    "score1=xgb.score(x_train,y_train)\n",
    "score1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 1, 4, 3, 3, 0, 0, 1, 2, 0, 1, 0, 0, 0, 3, 1, 1, 1, 3, 4, 4,\n",
       "       0, 0, 1, 4, 0, 0, 1, 0, 3, 0, 2, 3, 1, 1, 2, 3, 1, 4, 0, 0, 0, 2,\n",
       "       2, 3, 4, 0, 4, 3, 0, 3, 2, 3, 0, 4, 4, 0, 3, 1, 3, 4, 0, 0, 1, 0,\n",
       "       3, 2, 4, 1, 3, 4, 3, 2, 2, 1, 1, 4, 1, 1, 4, 4, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=xgb.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['warrior2', 'goddess', 'downdog', 'warrior2', 'plank', 'plank',\n",
       "       'Normal', 'Normal', 'downdog', 'goddess', 'Normal', 'downdog',\n",
       "       'Normal', 'Normal', 'Normal', 'plank', 'downdog', 'downdog',\n",
       "       'downdog', 'plank', 'warrior2', 'warrior2', 'Normal', 'Normal',\n",
       "       'downdog', 'warrior2', 'Normal', 'Normal', 'downdog', 'Normal',\n",
       "       'plank', 'Normal', 'goddess', 'plank', 'downdog', 'downdog',\n",
       "       'goddess', 'plank', 'downdog', 'warrior2', 'Normal', 'Normal',\n",
       "       'Normal', 'goddess', 'goddess', 'plank', 'warrior2', 'Normal',\n",
       "       'warrior2', 'plank', 'Normal', 'plank', 'goddess', 'plank',\n",
       "       'Normal', 'warrior2', 'warrior2', 'Normal', 'plank', 'downdog',\n",
       "       'plank', 'warrior2', 'Normal', 'Normal', 'downdog', 'Normal',\n",
       "       'plank', 'goddess', 'warrior2', 'downdog', 'plank', 'warrior2',\n",
       "       'plank', 'goddess', 'goddess', 'downdog', 'downdog', 'warrior2',\n",
       "       'downdog', 'downdog', 'warrior2', 'warrior2', 'Normal', 'Normal'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Yoga_ML.pkl','wb') as f:\n",
    "    pickle.dump(xgb,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adm88\\ML_deployment\\Object_detection\\yoga_pose_media_pipe\\pose.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000018?line=0'>1</a>\u001b[0m \u001b[39m# Get Camera feed\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000018?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mYoga_ML.pkl\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000018?line=2'>3</a>\u001b[0m     model_gbc\u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000018?line=3'>4</a>\u001b[0m video\u001b[39m=\u001b[39mcv\u001b[39m.\u001b[39mVideoCapture(\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000018?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m mp_pose\u001b[39m.\u001b[39mPose(model_complexity\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,min_detection_confidence\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,min_tracking_confidence\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m) \u001b[39mas\u001b[39;00m pose:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adm88/ML_deployment/Object_detection/yoga_pose_media_pipe/pose.ipynb#ch0000018?line=6'>7</a>\u001b[0m     \u001b[39m# set time \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Get Camera feed\n",
    "with open('Yoga_ML.pkl','rb') as f:\n",
    "    model_gbc= pickle.load(f)\n",
    "video=cv.VideoCapture(0)\n",
    "\n",
    "with mp_pose.Pose(model_complexity=2,min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "    # set time \n",
    "    time1= 0\n",
    "\n",
    "\n",
    "    while video.isOpened():\n",
    "        ret, frame= video.read()\n",
    "        frame.flags.writeable=False\n",
    "        \n",
    "\n",
    "        # change color feed\n",
    "        \n",
    "        img=cv.cvtColor(frame,cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        img=cv.flip(img,1)\n",
    "        # make detections\n",
    "        result=pose.process(img)\n",
    "\n",
    "        time2= time.time()\n",
    "\n",
    "        if (time2 - time1)>0:\n",
    "            \n",
    "            fps=round(1.0/(time2-time1),1)\n",
    "\n",
    "            cv.putText(img,f'FPS : {fps}',(10,30),cv.FONT_HERSHEY_PLAIN,2,(0,255,0),2)\n",
    "        \n",
    "\n",
    "        time1=time2\n",
    "        img.flags.writeable=True\n",
    "        img =cv.cvtColor(img,cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # Pose Detections\n",
    "        Class_landmarks= result.pose_landmarks.landmark\n",
    "        landmarks= list(np.array([[i.x,i.y,i.x,i.visibility] for i in Class_landmarks]).flatten())\n",
    "        mp_drawing.draw_landmarks(img, result.pose_landmarks, mp_pose.POSE_CONNECTIONS,mp_drawing.DrawingSpec(color=(178, 247, 31),thickness=2,circle_radius=1),mp_drawing.DrawingSpec(color=(129, 102, 108),thickness=1))\n",
    "        A=pd.DataFrame([landmarks])\n",
    "    \n",
    "        scaled_A=StandardScaler().fit_transform(A)\n",
    "        Face_body_class= model_gbc.predict(A)\n",
    "        texts=label.inverse_transform(Face_body_class)[0]\n",
    "        prob_class=model_gbc.predict_proba(A)[0]\n",
    "        print(texts,prob_class)\n",
    "\n",
    "\n",
    "        cv.rectangle(img,(20,100),(40,200),(0, 191, 255),-1)\n",
    "        cv.putText(img,texts,(200,30),cv.FONT_HERSHEY_PLAIN, 2,(255, 0, 255),2,cv.LINE_AA)\n",
    "        \n",
    "        cv.imshow('Raw Webcam Feed', img)\n",
    "\n",
    "\n",
    "\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5db15f2ea1626ef91825a72d4c48f0bc4b9025ec464b971927ebd6a1929834c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
